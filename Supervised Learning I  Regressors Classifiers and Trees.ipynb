{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "038513f2",
   "metadata": {},
   "source": [
    "# Transforming Data into Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d54b0de",
   "metadata": {},
   "source": [
    "Transforming Data into Features\n",
    "You are a data scientist at a clothing company and are working with a data set of customer reviews. This dataset is originally from Kaggle and has a lot of potential for various machine learning purposes. You are tasked with transforming some of these features to make the data more useful for analysis. To do this, you will have time to practice the following:\n",
    "\n",
    "Transforming categorical data\n",
    "Scaling your data\n",
    "Working with date-time features\n",
    "Let’s get started!\n",
    "\n",
    "Tasks\n",
    "16/16 complete\n",
    "Mark the tasks as complete by checking them off\n",
    "Basic Exploration\n",
    "1.\n",
    "Let’s start with some basic exploring by performing the following:\n",
    "\n",
    "First, import your dataset. It is stored under a file named reviews.csv. Save it to a variable called reviews.\n",
    "\n",
    "2.\n",
    "Next, we want to look at the column names of our dataset along with their data types. Do the following two steps:\n",
    "\n",
    "Print the column names of your dataset.\n",
    "Check your features’ data types by printing .info().\n",
    "Data Transformations\n",
    "3.\n",
    "Transform the recommended feature. Start by printing the feature’s .value_counts().\n",
    "\n",
    "4.\n",
    "Since this is a True/False feature, we want to transform it to 1 for True and 0 for False.\n",
    "\n",
    "To do this, create a dictionary called binary_dict where:\n",
    "\n",
    "The keys are what is currently in the recommended feature.\n",
    "The values are what we want in the new column (0s and 1s).\n",
    "Click the hint if you get stuck.\n",
    "\n",
    "5.\n",
    "Using binary_dict, transform the recommended column so that it will now be binary. Print the results using .value_counts() to confirm the transformation.\n",
    "\n",
    "6.\n",
    "Let’s run through a similar process to transform the rating feature. This is ordinal data so our transformation should make that more clear. Again, start by printing the .value_counts().\n",
    "\n",
    "To check your output, click the hint.\n",
    "\n",
    "7.\n",
    "We want to make the following changes to the values:\n",
    "\n",
    "‘Loved it’ → 5\n",
    "‘Liked it’ → 4\n",
    "‘Was okay’ → 3\n",
    "‘Not great’ → 2\n",
    "‘Hated it’ → 1\n",
    "Create a dictionary called rating_dict where the keys are what is currently in the feature and the values are what we want in the new column. You can use the hierarchy listed above to make your dictionary.\n",
    "\n",
    "8.\n",
    "Using rating_dict, transform the rating column so it contains numerical values. Print the results using .value_counts() to confirm the transformation.\n",
    "\n",
    "9.\n",
    "Let’s now transform the department_name feature. This process will be slightly different, but start by printing the .value_counts() of the feature.\n",
    "\n",
    "Use Panda’s get_dummies to one-hot encode our feature.\n",
    "Attach the results back to our original data frame.\n",
    "Print the column names to see!\n",
    "10.\n",
    "Use panda’s get_dummies() method to one-hot encode our feature. Assign this to a variable called one_hot.\n",
    "\n",
    "11.\n",
    "Join the results from one_hot back to our original data frame. Then print out the column names. What has been added?\n",
    "\n",
    "12.\n",
    "Let’s make one more feature transformation!\n",
    "\n",
    "Transform the review_date feature.\n",
    "\n",
    "This feature is listed as an object type, but we want this to be transformed into a date-time feature.\n",
    "\n",
    "Transform review_date into a date-time feature.\n",
    "Print the feature type to confirm the transformation.\n",
    "Click the hint if you get stuck.\n",
    "\n",
    "Scaling the Data\n",
    "13.\n",
    "The final step we will take in our transformation project is scaling our data. We notice that we have a wide range of numbers thus far, so it is best to put everything on the same scale.\n",
    "\n",
    "Let’s get our data frame to only have the numerical features we created. If you get stuck, click the hint.\n",
    "\n",
    "14.\n",
    "Reset the index to be our clothing_id feature.\n",
    "\n",
    "15.\n",
    "We are ready to scale our data! Perform a .fit_transform() on our data set, and print the results to see how the features have changed.\n",
    "\n",
    "16.\n",
    "Congratulations!\n",
    "\n",
    "You have successfully completed this transformation project. Transformations are an incredibly valuable skill to have. Great job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 1. Load the dataset from 'reviews.csv' into a DataFrame called reviews\n",
    "reviews = pd.read_csv('reviews.csv')\n",
    "\n",
    "# 2. Print column names and data types\n",
    "print(\"Column Names:\", reviews.columns)\n",
    "print(\"\\nData Types:\")\n",
    "print(reviews.info())\n",
    "\n",
    "# 3. Print value counts of the 'recommended' feature\n",
    "print(\"\\nRecommended Value Counts:\")\n",
    "print(reviews['recommended'].value_counts())\n",
    "\n",
    "# 4. Create a dictionary to map True/False to 1/0\n",
    "binary_dict = {True: 1, False: 0}\n",
    "\n",
    "# 5. Apply the binary_dict to transform 'recommended' and confirm\n",
    "reviews['recommended'] = reviews['recommended'].map(binary_dict)\n",
    "print(\"\\nTransformed Recommended Value Counts:\")\n",
    "print(reviews['recommended'].value_counts())\n",
    "\n",
    "# 6. Print value counts of the 'rating' feature\n",
    "print(\"\\nRating Value Counts:\")\n",
    "print(reviews['rating'].value_counts())\n",
    "\n",
    "# 7. Create a dictionary to map rating text to ordinal values\n",
    "rating_dict = {\n",
    "    'Loved it': 5,\n",
    "    'Liked it': 4,\n",
    "    'Was okay': 3,\n",
    "    'Not great': 2,\n",
    "    'Hated it': 1\n",
    "}\n",
    "\n",
    "# 8. Apply the rating_dict to transform 'rating' and confirm\n",
    "reviews['rating'] = reviews['rating'].map(rating_dict)\n",
    "print(\"\\nTransformed Rating Value Counts:\")\n",
    "print(reviews['rating'].value_counts())\n",
    "\n",
    "# 9. Print value counts of 'department_name' before one-hot encoding\n",
    "print(\"\\nDepartment Name Value Counts:\")\n",
    "print(reviews['department_name'].value_counts())\n",
    "\n",
    "# 10. One-hot encode 'department_name' and store in one_hot\n",
    "one_hot = pd.get_dummies(reviews['department_name'], prefix='dept')\n",
    "\n",
    "# 11. Join one_hot back to reviews and print new column names\n",
    "reviews = reviews.join(one_hot)\n",
    "print(\"\\nUpdated Column Names After One-Hot Encoding:\")\n",
    "print(reviews.columns)\n",
    "\n",
    "# 12. Convert 'review_date' to datetime format and confirm\n",
    "reviews['review_date'] = pd.to_datetime(reviews['review_date'])\n",
    "print(\"\\nReview Date Data Type After Conversion:\")\n",
    "print(reviews['review_date'].dtype)\n",
    "\n",
    "# 13. Select only numerical features for scaling\n",
    "numerical_features = reviews[['clothing_id', 'recommended', 'rating'] + list(one_hot.columns)]\n",
    "\n",
    "# 14. Reset index to 'clothing_id'\n",
    "numerical_features.set_index('clothing_id', inplace=True)\n",
    "\n",
    "# 15. Scale the data using MinMaxScaler and print results\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_features)\n",
    "print(\"\\nScaled Data Sample:\")\n",
    "print(pd.DataFrame(scaled_data, columns=numerical_features.columns).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66804fbc",
   "metadata": {},
   "source": [
    "# Supervised Learning I : Regressors, Classifiers and Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62617e70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1973196795.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Machine Learning/AI Engineer\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Machine Learning/AI Engineer\n",
    "Honey Production\n",
    "Now that you have learned how linear regression works, let’s try it on an example of real-world data.\n",
    "\n",
    "As you may have already heard, the honeybees are in a precarious state right now. You may have seen articles about the decline of the honeybee population for various reasons. You want to investigate this decline and how the trends of the past predict the future for the honeybees.\n",
    "\n",
    "Note: All the tasks can be completed using Pandas or NumPy. Pick whichever one you prefer.\n",
    "\n",
    "If you get stuck during this project or would like to see an experienced developer work through it, click “Get Unstuck“ to see a project walkthrough video.\n",
    "\n",
    "Tasks\n",
    "13/13 complete\n",
    "Mark the tasks as complete by checking them off\n",
    "Check out the Data\n",
    "1.\n",
    "We have loaded in a DataFrame for you about honey production in the United States from Kaggle. It is called df and has the following columns:\n",
    "\n",
    "state\n",
    "numcol\n",
    "yieldpercol\n",
    "totalprod\n",
    "stocks\n",
    "priceperlb\n",
    "prodvalue\n",
    "year\n",
    "Use .head() to get a sense of how this DataFrame is structured.\n",
    "\n",
    "2.\n",
    "For now, we care about the total production of honey per year. Use the .groupby() method provided by pandas to get the mean of totalprod per year.\n",
    "\n",
    "Store this in a variable called prod_per_year.\n",
    "\n",
    "3.\n",
    "Create a variable called X that is the column of years in this prod_per_year DataFrame.\n",
    "\n",
    "After creating X, we will need to reshape it to get it into the right format, using this command:\n",
    "\n",
    "X = X.values.reshape(-1, 1)\n",
    "\n",
    "Copy to Clipboard\n",
    "\n",
    "4.\n",
    "Create a variable called y that is the totalprod column in the prod_per_year dataset.\n",
    "\n",
    "5.\n",
    "Using plt.scatter(), plot y vs X as a scatterplot.\n",
    "\n",
    "Display the plot using plt.show().\n",
    "\n",
    "Can you see a vaguely linear relationship between these variables?\n",
    "\n",
    "Create and Fit a Linear Regression Model\n",
    "6.\n",
    "Create a linear regression model from scikit-learn and call it regr.\n",
    "\n",
    "Use the LinearRegression() constructor from the linear_model module to do this.\n",
    "\n",
    "7.\n",
    "Fit the model to the data by using .fit(). You can feed X into your regr model by passing it in as a parameter of .fit().\n",
    "\n",
    "8.\n",
    "After you have fit the model, print out the slope of the line (stored in a list called regr.coef_) and the intercept of the line (regr.intercept_).\n",
    "\n",
    "9.\n",
    "Create a list called y_predict that is the predictions your regr model would make on the X data.\n",
    "\n",
    "10.\n",
    "Plot y_predict vs X as a line, on top of your scatterplot using plt.plot().\n",
    "\n",
    "Make sure to call plt.show() after plotting the line.\n",
    "\n",
    "Predict the Honey Decline\n",
    "11.\n",
    "So, it looks like the production of honey has been in decline, according to this linear model. Let’s predict what the year 2050 may look like in terms of honey production.\n",
    "\n",
    "Our known dataset stops at the year 2013, so let’s create a NumPy array called X_future that is the range from 2013 to 2050. The code below makes a NumPy array with the numbers 1 through 10\n",
    "\n",
    "nums = np.array(range(1, 11))\n",
    "\n",
    "Copy to Clipboard\n",
    "\n",
    "After creating that array, we need to reshape it for scikit-learn.\n",
    "\n",
    "X_future = X_future.reshape(-1, 1)\n",
    "\n",
    "Copy to Clipboard\n",
    "\n",
    "You can think of reshape() as rotating this array. Rather than one big row of numbers, X_future is now a big column of numbers — there’s one number in each row.\n",
    "\n",
    "reshape() is a little tricky! It might help to print out X_future before and after reshaping.\n",
    "\n",
    "12.\n",
    "Create a list called future_predict that is the y-values that your regr model would predict for the values of X_future.\n",
    "\n",
    "13.\n",
    "Plot future_predict vs X_future on a different plot.\n",
    "\n",
    "How much honey will be produced in the year 2050, according to this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 1. Load the dataset from the provided URL\n",
    "df = pd.read_csv(\"https://content.codecademy.com/programs/data-science-path/linear_regression/honeyproduction.csv\")\n",
    "\n",
    "# 2. Group by year and calculate the mean of total production\n",
    "prod_per_year = df.groupby('year')['totalprod'].mean().reset_index()\n",
    "\n",
    "# 3. Extract the 'year' column and reshape it for regression\n",
    "X = prod_per_year['year'].values.reshape(-1, 1)\n",
    "\n",
    "# 4. Extract the 'totalprod' column as the target variable\n",
    "y = prod_per_year['totalprod'].values\n",
    "\n",
    "# 5. Plot the data to visualize the relationship\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Honey Production')\n",
    "plt.title('Honey Production Over Time')\n",
    "plt.show()\n",
    "\n",
    "# 6. Create a linear regression model\n",
    "regr = LinearRegression()\n",
    "\n",
    "# 7. Fit the model to the data\n",
    "regr.fit(X, y)\n",
    "\n",
    "# 8. Print the slope and intercept of the regression line\n",
    "print(\"Slope (Coefficient):\", regr.coef_[0])\n",
    "print(\"Intercept:\", regr.intercept_)\n",
    "\n",
    "# 9. Predict y values using the trained model\n",
    "y_predict = regr.predict(X)\n",
    "\n",
    "# 10. Plot the regression line over the scatterplot\n",
    "plt.scatter(X, y, label='Actual')\n",
    "plt.plot(X, y_predict, color='red', label='Regression Line')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Honey Production')\n",
    "plt.title('Honey Production with Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 11. Create future years from 2013 to 2050 and reshape\n",
    "X_future = np.array(range(2013, 2051)).reshape(-1, 1)\n",
    "\n",
    "# 12. Predict future honey production\n",
    "future_predict = regr.predict(X_future)\n",
    "\n",
    "# 13. Plot future predictions\n",
    "plt.plot(X_future, future_predict, color='green')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Predicted Honey Production')\n",
    "plt.title('Predicted Honey Production (2013–2050)')\n",
    "plt.show()\n",
    "\n",
    "# Print predicted production for 2050\n",
    "print(\"Predicted honey production in 2050:\", future_predict[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd063175",
   "metadata": {},
   "source": [
    "Tennis Ace\n",
    "Overview\n",
    "This project contains a series of open-ended requirements which describe the project you’ll be building. There are many possible ways to correctly fulfill all of these requirements, and you should expect to use the internet, Codecademy, and other resources when you encounter a problem.\n",
    "\n",
    "Project Goals\n",
    "You will create a linear regression model that predicts the outcome for a tennis player based on their playing habits. By analyzing and modeling the Association of Tennis Professionals (ATP) data, you will determine what it takes to be one of the best tennis players in the world.\n",
    "\n",
    "Setup Instructions\n",
    "If you choose to do this project on your computer instead of Codecademy, you can download what you’ll need by clicking the “Download” button below. If you need help setting up your computer, be sure to check out our setup guide.\n",
    "\n",
    "Download\n",
    "Tasks\n",
    "7/8 complete\n",
    "Mark the tasks as complete by checking them off\n",
    "Prerequisites\n",
    "1.\n",
    "In order to complete this project, you should have completed the Linear Regression and Multiple Linear Regression lessons in the Machine Learning Course. This content is also covered in the Data Scientist Career Path.\n",
    "\n",
    "Project Requirements\n",
    "2.\n",
    "“Game, Set, Match!”\n",
    "\n",
    "No three words are sweeter to hear as a tennis player than those, which indicate that a player has beaten their opponent. While you can head down to your nearest court and aim to overcome your challenger across the net without much practice, a league of professionals spends day and night, month after month practicing to be among the best in the world. Today you will put your linear regression knowledge to the test to better understand what it takes to be an all-star tennis player.\n",
    "\n",
    "Provided in tennis_stats.csv is data from the men’s professional tennis league, which is called the ATP (Association of Tennis Professionals). Data from the top 1500 ranked players in the ATP over the span of 2009 to 2017 are provided in file. The statistics recorded for each player in each year include service game (offensive) statistics, return game (defensive) statistics and outcomes. Load the csv into a DataFrame and investigate it to gain familiarity with the data.\n",
    "\n",
    "Open the hint for more information about each column of the dataset.\n",
    "\n",
    "3.\n",
    "Perform exploratory analysis on the data by plotting different features against the different outcomes. What relationships do you find between the features and outcomes? Do any of the features seem to predict the outcomes?\n",
    "\n",
    "4.\n",
    "Use one feature from the dataset to build a single feature linear regression model on the data. Your model, at this point, should use only one feature and predict one of the outcome columns. Before training the model, split your data into training and test datasets so that you can evaluate your model on the test set. How does your model perform? Plot your model’s predictions on the test set against the actual outcome variable to visualize the performance.\n",
    "\n",
    "5.\n",
    "Create a few more linear regression models that use one feature to predict one of the outcomes. Which model that you create is the best?\n",
    "\n",
    "6.\n",
    "Create a few linear regression models that use two features to predict yearly earnings. Which set of two features results in the best model?\n",
    "\n",
    "7.\n",
    "Create a few linear regression models that use multiple features to predict yearly earnings. Which set of features results in the best model?\n",
    "\n",
    "Head to the Codecademy forums and share your set of features that resulted in the highest test score for predicting your outcome. What features are most important for being a successful tennis player?\n",
    "\n",
    "Solution\n",
    "8.\n",
    "Great work! Visit our forums to compare your project to our sample solution code. You can also learn how to host your own solution on GitHub so you can share it with other learners! Your solution might look different from ours, and that’s okay! There are multiple ways to solve these projects, and you’ll learn more by seeing others’ code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a2181",
   "metadata": {},
   "source": [
    "# Tennis Ace => see download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76038994",
   "metadata": {},
   "source": [
    "Predict Credit Card Fraud\n",
    "Credit card fraud is one of the leading causes of identify theft around the world. In 2018 alone, over $24 billion were stolen through fraudulent credit card transactions. Financial institutions employ a wide variety of different techniques to prevent fraud, one of the most common being Logistic Regression.\n",
    "\n",
    "In this project, you are a Data Scientist working for a credit card company. You have access to a dataset (based on a synthetic financial dataset), that represents a typical set of credit card transactions. transactions.csv is the original dataset containing 200k transactions. For starters, we’re going to be working with a small portion of this dataset, transactions_modified.csv, which contains one thousand transactions. Your task is to use Logistic Regression and create a predictive model to determine if a transaction is fraudulent or not.\n",
    "\n",
    "Note that a solution.py file is loaded for you in the workspace, which contains solution code for this project. We highly recommend that you complete the project on your own without checking the solution, but feel free to take a look if you get stuck or want to check your answers when you’re done!\n",
    "\n",
    "Tasks\n",
    "17/17 complete\n",
    "Mark the tasks as complete by checking them off\n",
    "Load the Data\n",
    "1.\n",
    "The file transactions_modified.csv contains data on 1000 simulated credit card transactions. Let’s begin by loading the data into a pandas DataFrame named transactions. Take a peek at the dataset using .head() and you can use .info() to examine how many rows are there and what datatypes the are. How many transactions are fraudulent? Print your answer.\n",
    "\n",
    "Clean the Data\n",
    "2.\n",
    "Looking at the dataset, combined with our knowledge of credit card transactions in general, we can see that there are a few interesting columns to look at. We know that the amount of a given transaction is going to be important. Calculate summary statistics for this column. What does the distribution look like?\n",
    "\n",
    "3.\n",
    "We have a lot of information about the type of transaction we are looking at. Let’s create a new column called isPayment that assigns a 1 when type is “PAYMENT” or “DEBIT”, and a 0 otherwise.\n",
    "\n",
    "4.\n",
    "Similarly, create a column called isMovement, which will capture if money moved out of the origin account. This column will have a value of 1 when type is either “CASH_OUT” or “TRANSFER”, and a 0 otherwise.\n",
    "\n",
    "5.\n",
    "With financial fraud, another key factor to investigate would be the difference in value between the origin and destination account. Our theory, in this case, being that destination accounts with a significantly different value could be suspect of fraud. Let’s create a column called accountDiff with the absolute difference of the oldbalanceOrg and oldbalanceDest columns.\n",
    "\n",
    "Select and Split the Data\n",
    "6.\n",
    "Before we can start training our model, we need to define our features and label columns. Our label column in this dataset is the isFraud field. Create a variable called features which will be an array consisting of the following fields:\n",
    "\n",
    "amount\n",
    "isPayment\n",
    "isMovement\n",
    "accountDiff\n",
    "Also create a variable called label with the column isFraud.\n",
    "\n",
    "7.\n",
    "Split the data into training and test sets using sklearn‘s train_test_split() method. We’ll use the training set to train the model and the test set to evaluate the model. Use a test_size value of 0.3.\n",
    "\n",
    "Normalize the Data\n",
    "8.\n",
    "Since sklearn‘s Logistic Regression implementation uses Regularization, we need to scale our feature data. Create a StandardScaler object, .fit_transform() it on the training features, and .transform() the test features.\n",
    "\n",
    "Create and Evaluate the Model\n",
    "9.\n",
    "Create a LogisticRegression model with sklearn and .fit() it on the training data.\n",
    "\n",
    "Fitting the model find the best coefficients for our selected features so it can more accurately predict our label. We will start with the default threshold of 0.5.\n",
    "\n",
    "10.\n",
    "Run the model’s .score() method on the training data and print the training score.\n",
    "\n",
    "Scoring the model on the training data will process the training data through the trained model and will predict which transactions are fraudulent. The score returned is the percentage of correct classifications, or the accuracy.\n",
    "\n",
    "11.\n",
    "Run the model’s .score() method on the test data and print the test score.\n",
    "\n",
    "Scoring the model on the test data will process the test data through the trained model and will predict which transactions are fraudulent. The score returned is the percentage of correct classifications, or the accuracy, and will be an indicator for the sucess of your model.\n",
    "\n",
    "How did your model perform?\n",
    "\n",
    "12.\n",
    "Print the coefficients for our model to see how important each feature column was for prediction. Which feature was most important? Least important?\n",
    "\n",
    "Predict With the Model\n",
    "13.\n",
    "Let’s use our model to process more transactions that have gone through our systems. There are three numpy arrays pre-loaded in the workspace with information on new sample transactions under “New transaction data”\n",
    "\n",
    "# New transaction data\n",
    "transaction1 = np.array([123456.78, 0.0, 1.0, 54670.1])\n",
    "transaction2 = np.array([98765.43, 1.0, 0.0, 8524.75])\n",
    "transaction3 = np.array([543678.31, 1.0, 0.0, 510025.5])\n",
    "\n",
    "Copy to Clipboard\n",
    "\n",
    "Create a fourth array, your_transaction, and add any transaction information you’d like. Make sure to enter all values as floats with a .!\n",
    "\n",
    "14.\n",
    "Combine the new transactions and your_transaction into a single numpy array called sample_transactions.\n",
    "\n",
    "15.\n",
    "Since our Logistic Regression model was trained on scaled feature data, we must also scale the feature data we are making predictions on. Using the StandardScaler object created earlier, apply its .transform() method to sample_transactions and save the result to sample_transactions.\n",
    "\n",
    "16.\n",
    "Which transactions are fraudulent? Use your model’s .predict() method on sample_transactions and print the result to find out.\n",
    "\n",
    "Want to see the probabilities that led to these predictions? Call your model’s .predict_proba() method on sample_transactions and print the result. The 1st column is the probability of a transaction not being fraudulent, and the 2nd column is the probability of a transaction being fraudulent (which was calculated by our model to make the final classification decision).\n",
    "\n",
    "17.\n",
    "Congratulations on completing the project!\n",
    "\n",
    "Note that we’d used a modified version of the dataset. You can now try to re-run the project using the original dataset, transactions.csv. Examine how the results change. If you notice something weird, you’re totally on to something! That “something” is what is known as an imbalanced class classification problem.\n",
    "\n",
    "We will cover this very relevant topic (among many other things) in the Logistic Regression II module!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98687740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import codecademylib3\n",
    "\n",
    "# Load the data\n",
    "transactions = pd.read_csv('transactions_modified.csv')\n",
    "print(transactions.head())\n",
    "print(transactions.info())\n",
    "\n",
    "# Summary statistics on amount column\n",
    "transactions['amount'].describe()\n",
    "\n",
    "# Create isPayment field\n",
    "transactions['isPayment'] = 0\n",
    "transactions['isPayment'][transactions['type'].isin(['PAYMENT','DEBIT'])] = 1\n",
    "\n",
    "# Create isMovement field\n",
    "transactions['isMovement'] = 0\n",
    "transactions['isMovement'][transactions['type'].isin(['CASH_OUT', 'TRANSFER'])] = 1\n",
    "\n",
    "# Create accountDiff field\n",
    "transactions['accountDiff'] = abs(transactions['oldbalanceDest'] - transactions['oldbalanceOrg'])\n",
    "\n",
    "# Create features and label variables\n",
    "features = transactions[['amount','isPayment','isMovement','accountDiff']]\n",
    "label = transactions['isFraud']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    label, \n",
    "                                                    test_size=0.3)\n",
    "\n",
    "# Normalize the features variables\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the training data\n",
    "print(model.score(X_train, y_train))\n",
    "\n",
    "# Score the model on the test data\n",
    "print(model.score(X_test, y_test))\n",
    "\n",
    "# Print the model coefficients\n",
    "print(model.coef_)\n",
    "\n",
    "# New transaction data\n",
    "transaction1 = np.array([123456.78, 0.0, 1.0, 54670.1])\n",
    "transaction2 = np.array([98765.43, 1.0, 0.0, 8524.75])\n",
    "transaction3 = np.array([543678.31, 1.0, 0.0, 510025.5])\n",
    "\n",
    "# Create a new transaction\n",
    "your_transaction = np.array([6472.54, 1.0, 0.0, 55901.23])\n",
    "\n",
    "# Combine new transactions into a single array\n",
    "sample_transactions = np.stack((transaction1,transaction2,transaction3,your_transaction))\n",
    "\n",
    "# Normalize the new transactions\n",
    "sample_transactions = scaler.transform(sample_transactions)\n",
    "\n",
    "# Predict fraud on the new transactions\n",
    "print(model.predict(sample_transactions))\n",
    "\n",
    "# Show probabilities on the new transactions\n",
    "print(model.predict_proba(sample_transactions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0de915",
   "metadata": {},
   "source": [
    "Cancer Classifier\n",
    "In this project, we will be using several Python libraries to make a K-Nearest Neighbor classifier that is trained to predict whether a patient has breast cancer.\n",
    "\n",
    "If you get stuck during this project or would like to see an experienced developer work through it, click “Get Unstuck“ to see a project walkthrough video.\n",
    "\n",
    "Tasks\n",
    "18/18 complete\n",
    "Mark the tasks as complete by checking them off\n",
    "Explore the data\n",
    "1.\n",
    "Let’s begin by importing the breast cancer data from sklearn. We want to import the function load_breast_cancer from sklearn.datasets.\n",
    "\n",
    "Once we’ve imported the dataset, let’s load the data into a variable called breast_cancer_data. Do this by setting breast_cancer_data equal to the function load_breast_cancer().\n",
    "\n",
    "2.\n",
    "Before jumping into creating our classifier, let’s take a look at the data. Begin by printing breast_cancer_data.data[0]. That’s the first datapoint in our set. But what do all of those numbers represent? Let’s also print breast_cancer_data.feature_names.\n",
    "\n",
    "3.\n",
    "We now have a sense of what the data looks like, but what are we trying to classify? Let’s print both breast_cancer_data.target and breast_cancer_data.target_names.\n",
    "\n",
    "Was the very first data point tagged as malignant or benign?\n",
    "\n",
    "Splitting the data into Training and Validation Sets\n",
    "4.\n",
    "We have our data, but now it needs to be split into training and validation sets. Luckily, sklearn has a function that does that for us. Begin by importing the train_test_split function from sklearn.model_selection.\n",
    "\n",
    "5.\n",
    "Call the train_test_split function. It takes several parameters:\n",
    "\n",
    "The data you want to split (for us breast_cancer_data.data)\n",
    "The labels associated with that data (for us, breast_cancer_data.target).\n",
    "The test_size. This is what percentage of your data you want to be in your testing set. Let’s use test_size = 0.2\n",
    "random_state. This will ensure that every time you run your code, the data is split in the same way. This can be any number. We used random_state = 100.\n",
    "6.\n",
    "Right now we’re not storing the return value of train_test_split. train_test_split returns four values in the following order:\n",
    "\n",
    "The training set\n",
    "The validation set\n",
    "The training labels\n",
    "The validation labels\n",
    "Store those values in variables named training_data, validation_data, training_labels, and validation_labels.\n",
    "\n",
    "7.\n",
    "Let’s confirm that worked correctly. Print out the length of training_data and training_labels. They should be the same size - one label for every piece of data!\n",
    "\n",
    "Running the classifier\n",
    "8.\n",
    "Now that we’ve created training and validation sets, we can create a KNeighborsClassifier and test its accuracy. Begin by importing KNeighborsClassifier from sklearn.neighbors.\n",
    "\n",
    "9.\n",
    "Create a KNeighborsClassifier where n_neighbors = 3. Name the classifier classifier.\n",
    "\n",
    "10.\n",
    "Train your classifier using the fit function. This function takes two parameters: the training set and the training labels.\n",
    "\n",
    "11.\n",
    "Now that the classifier has been trained, let’s find how accurate it is on the validation set. Call the classifier’s score function. score takes two parameters: the validation set and the validation labels. Print the result!\n",
    "\n",
    "12.\n",
    "The classifier does pretty well when k = 3. But maybe there’s a better k! Put the previous 3 lines of code inside a for loop. The loop should have a variable named k that starts at 1 and increases to 100. Rather than n_neighbors always being 3, it should be this new variable k.\n",
    "\n",
    "You should now see 100 different validation accuracies print out. Which k seems the best?\n",
    "\n",
    "Graphing the results\n",
    "13.\n",
    "We now have the validation accuracy for 100 different ks. Rather than just printing it out, let’s make a graph using matplotlib. Begin by importing matplotlib.pyplot as plt.\n",
    "\n",
    "14.\n",
    "The x-axis should be the values of k that we tested. This should be a list of numbers between 1 and 100. You can use the range function to make this list. Store it in a variable named k_list.\n",
    "\n",
    "15.\n",
    "The y-axis of our graph should be the validation accuracy. Instead of printing the validation accuracies, we want to add them to a list. Outside of the for loop, create an empty list named accuracies. Inside the for loop, instead of printing each accuracy, append it to accuracies.\n",
    "\n",
    "16.\n",
    "We can now plot our data! Call plt.plot(). The first parameter should be k_list and the second parameter should be accuracies.\n",
    "\n",
    "After plotting the graph, show it using plt.show().\n",
    "\n",
    "17.\n",
    "Let’s add some labels and a title. Set the x-axis label to \"k\" using plt.xlabel(). Set the y-axis label to \"Validation Accuracy\". Set the title to \"Breast Cancer Classifier Accuracy\".\n",
    "\n",
    "18.\n",
    "Great work! If you want to play around with this more, try changing the random_state parameter when making the training set and validation set. This will change which points are in the training set and which are in the validation set.\n",
    "\n",
    "Ideally, the graph will look the same no matter how you split up the training set and test set. This data set is fairly small, so there is slightly more variance than usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d08c0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecademylib3\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "#https://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.data\n",
    "cols = ['name','landmass','zone', 'area', 'population', 'language','religion','bars','stripes','colours',\n",
    "'red','green','blue','gold','white','black','orange','mainhue','circles',\n",
    "'crosses','saltires','quarters','sunstars','crescent','triangle','icon','animate','text','topleft','botright']\n",
    "df= pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/flags/flag.data\", names = cols)\n",
    "\n",
    "#variable names to use as predictors\n",
    "var = [ 'red', 'green', 'blue','gold', 'white', 'black', 'orange', 'mainhue','bars','stripes', 'circles','crosses', 'saltires','quarters','sunstars','triangle','animate']\n",
    "\n",
    "#Print number of countries by landmass, or continent\n",
    "print(df.landmass.value_counts())\n",
    "\n",
    "#Create a new dataframe with only flags from Europe and Oceania\n",
    "df_36 = df[df[\"landmass\"].isin([3,6])]\n",
    "\n",
    "#Print the average vales of the predictors for Europe and Oceania\n",
    "print(df_36.groupby('landmass')[var].mean().T)\n",
    "\n",
    "#Create labels for only Europe and Oceania\n",
    "df_36 = df[df[\"landmass\"].isin([3,6])]\n",
    "labels = df_36[\"landmass\"]\n",
    "\n",
    "#Print the variable types for the predictors\n",
    "print(df[var].dtypes)\n",
    "\n",
    "#Create dummy variables for categorical predictors\n",
    "data = pd.get_dummies(df_36[var])\n",
    "\n",
    "#Split data into a train and test set\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, random_state=1, test_size=.4)\n",
    "\n",
    "#Fit a decision tree for max_depth values 1-20; save the accuracy score in acc_depth\n",
    "depths = range(1, 21)\n",
    "acc_depth = []\n",
    "for i in depths:\n",
    "    dt = DecisionTreeClassifier(random_state = 10, max_depth = i)\n",
    "    dt.fit(train_data, train_labels)\n",
    "    acc_depth.append(dt.score(test_data, test_labels))\n",
    "\n",
    "#Plot the accuracy vs depth\n",
    "plt.plot(depths, acc_depth)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Find the largest accuracy and the depth this occurs\n",
    "max_acc = np.max(acc_depth)\n",
    "best_depth = depths[np.argmax(acc_depth)]\n",
    "print(f'Highest accuracy {round(max_acc,3)*100}% at depth {best_depth}')\n",
    "\n",
    "#Refit decision tree model with the highest accuracy and plot the decision tree\n",
    "plt.figure(figsize=(14,8))\n",
    "dt = DecisionTreeClassifier(random_state = 1, max_depth = best_depth)\n",
    "dt.fit(train_data, train_labels)\n",
    "tree.plot_tree(dt, feature_names = train_data.columns,  \n",
    "               class_names = ['Europe', 'Oceania'],\n",
    "                filled=True)\n",
    "plt.show()\n",
    "\n",
    "#Create a new list for the accuracy values of a pruned decision tree.  Loop through\n",
    "#the values of ccp and append the scores to the list\n",
    "acc_pruned = []\n",
    "ccp = np.logspace(-3, 0, num=20)\n",
    "for i in ccp:\n",
    "    dt_prune = DecisionTreeClassifier(random_state = 1, max_depth = best_depth, ccp_alpha=i)\n",
    "    dt_prune.fit(train_data, train_labels)\n",
    "    acc_pruned.append(dt_prune.score(test_data, test_labels))\n",
    "\n",
    "plt.plot(ccp, acc_pruned)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "\n",
    "#Find the largest accuracy and the ccp value this occurs\n",
    "max_acc_pruned = np.max(acc_pruned)\n",
    "best_ccp = ccp[np.argmax(acc_pruned)]\n",
    "\n",
    "print(f'Highest accuracy {round(max_acc_pruned,3)*100}% at ccp_alpha {round(best_ccp,4)}')\n",
    "\n",
    "#Fit a decision tree model with the values for max_depth and ccp_alpha found above\n",
    "dt_final = DecisionTreeClassifier(random_state = 1, max_depth = best_depth, ccp_alpha=best_ccp)\n",
    "dt_final.fit(train_data, train_labels)\n",
    "\n",
    "#Plot the final decision tree\n",
    "plt.figure(figsize=(14,8))\n",
    "tree.plot_tree(dt_final, feature_names = train_data.columns,  \n",
    "               class_names = ['Europe', 'Oceania'],\n",
    "                filled=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
